:scrollbar:
:data-uri:
:toc2:
:imagesdir: images

== Infrastructure Migration 1.2 GA - Migration Guide

:numbered:

== Overview

This Lab Environment uses Red Hat CloudForms, and the Infrastructure Migration plugin included with it, as a broker to migrate VMs from VMware vSphere to Red Hat Virtualization (RHV). During the migration, supported OSes can be converted to RHEL and get registered to Satellite for seamless management automatically. Additionally,
the migrated and registered VMs can be included to Red Hat Insights inventory. Red Hat Insights helps your infrastructure by proactively identifying and suggesting remediation steps to increase security, performance and stability.  

The password and user accounts to access all services is provided by the lab administrators.

This environment consists of the following systems deployed, along with the necessary configuration:

* Deployed RHV and VMware providers, Cloud Forms and Satellite.
* Configured the conversion hosts in the target provider (Red Hat Virtualization)
* Credentials for virtualization providers and conversion hosts in CloudForms are added
* Assigned tags for the conversion hosts in CloudForms
* Registered Satellite to Red Hat Insights

The providers are:

* Source - VMware vSphere
* Target - Red Hat Virtualization

Infrastructure management provided by:

* Red Hat Satellite 6.6
* CloudForms Management Engine

At the beginning, the resource equivalence between the two providers involved in each migration, has to be defined in an *Infrastucture Mapping* including:

* Clusters
* Storage Domains
* Networks

To perform the migration two *Conversion Hosts* were configured for the target provider. The roles of *Conversion Hosts* are fulfilled by RHV hypervisors. In the case of this lab those are kvm1.example.com and kvm2.example.com.

.Goal
* Migrate and convert several VMs from VMware vSphere to Red Hat Virtualization, using Red Hat CloudForms as a broker, with the tooling included in the Red Hat Infrastructure Migration Solution. 
* Remediate issues found by Insights, by running included remediation ansible playbooks.

We will have to configure the needed *Infrastructure Mapping* and finally, add a *Migration Plan* to perform the final migration.

[NOTE]
The source VMs are never deleted, only powered off once the migration is done. This measure can also be used as "rollback" in case of a potential migration failure.

Versions of products used:

[cols="1,1",options="header"]
|=======
|Product |Version
|CloudForms |5.11.4
|Red Hat Virtualization |4.3.3
|Red Hat Enterprise Linux (Conversion Host) |7.6+
|Conversion Host Image |1.7+
|VMware vSphere |6.7
|=======

== Requirements running and accessing this lab

=== Base requirements

* A computer with access to the Internet :-)
* SSH client (for Microsoft Windows users, link:https://www.putty.org/[Putty] application or built-in bash in Windows 10, are both good choices)
* Firefox 17 or higher, or Chromium / Chrome
* To fully experience this lab, it is necessary to set up proxy for the duration. 
- That said, you _can_ do the lab _without a proxy_, but you will not be able to access some parts of the environment. This could affect your lab experience somewhat. 

==== Proxy and FoxyProxy plugin setup

To make the environment unique and distinguishable between attendees, a 5 character identifier is assigned to it (i.e. `r1e37`). This identifier will be referred to, within the documentation as *<YOUR-GUID>*. + 
You can set up a proxy by it in your browser to `workstation-*<YOUR-GUID>*.rhpds.opentlc.com`, port 3128. +
Using FoxyProxy is described in more details below.
When setting the proxy up, the proxy server should be used for all protocols in order to access the environment.

If you are using FoxyProxy here are the relevant settings:

[cols="1,2",options="header"]
|=======
| Option | Value
| Proxy IP address or DNS name: | `workstation-*<YOUR-GUID>*.rhdps.opentlc.com`
|Port|3128
|=======
*Foxy Proxy White Pattern list:*
[cols="1,1,1",options="header"]
|=======
|Name|Pattern|Type
|example.com|*.example.com|Wildcard
|192.168|192.168.0.*|Wildcard
|=======

When you configure FoxyProxy this way, only the patterns pertaining to the lab will be going through the proxy, the rest of your internet requests will use your default internet access.

[NOTE]
Grammarly plugin for Chrome may cause problems when managing CloudForms. If you have it in your browser, please deactivate it while doing this lab.

== Environment

A completely clean environment is deployed for your use. To make the environment unique and distinguishable between attendees, a 5 character identifier is assigned to it (i.e. `r1e37`). This identifier will be referred to, within the documentation as *<YOUR-GUID>*. 
So if we say to use `cf-<YOUR-GUID>.example.com`, that would translate to `cf-r1e37.example.com`, for example. Make sure you have this number at hand throughout the lab.

The migration environment consists of the following systems:

image::blueprint1.png[Blueprint]


[cols="1,1,1,2",options="header"]
|=======
| Hostname | Internal IP | External name | Description
|`workstation.example.com` |`192.168.0.10` | workstation-<YOUR-GUID>.rhpds.opentlc.com |Jump host, proxy and Ansible host
|`storage.example.com` |`192.168.0.254` | workstation-<YOUR-GUID>.rhpds.opentlc.com | NFS server
|`cf.example.com` |`192.168.0.100` |  cf-<YOUR-GUID>.rhpds.opentlc.com |CloudForms server
|`satellite.example.com` |`192.168.0.31` |  satellite-<YOUR-GUID>.rhpds.opentlc.com |Satellite 6.6 server
|`rhvm.example.com` |`192.168.0.35` | *Only Through Proxy* |Red Hat Virtualization Manager server
|`kvm1.example.com` |`192.168.0.41` | N/A  |KVM hypervisor managed by Red Hat Virtualization
|`kvm2.example.com` |`192.168.0.42` | N/A |KVM hypervisor managed by Red Hat Virtualization
|`vcenter.example.com` |`192.168.0.50` | *Only Through Proxy* |VMware vCenter server
|`esx1.example.com` |`192.168.0.51` | N/A |ESXi hypervisor
|`esx2.example.com` |`192.168.0.52` | N/A |ESXi hypervisor
|=======


The architecture of the deployment can be depicted with the following image:

image::architecture_diagram1.png[Architecture Diagram]

* Networks used in the environment

[cols="1,1,2",options="header"]
|=======
| Network Name | IP range | Description
| `Admin` | `192.168.x.x/16` | General administration and storage network.
| `Service` | `10.10.0.x/24` | Internal network for the VMs delivering services to users.
|=======

* Virtual Machines
+
This deployment of the migration environment includes the following VMs provisioned and running in the vSphere environment, which will be migrated:

[cols="1,1,2",options="header"]
|=======
| Name | IPs | Description
| `hana.example.com` | 10.10.0.130 | SAP HANA running on Red Hat Enterprise Linux 7 host.
| `oracledb.example.com` | 10.10.0.160 | Oracle DB running on Oracle Linux 7 host.
| `tomcat.example.com` | 10.10.0.180 | Tomcat running on CentOS Linux 7 host.
|=======


== Getting Started

. Once the system is running, use SSH to access your bastion workstation using `lab-user`, and the hostname `workstation-*YOUR_GUID*.rhpds.opentlc.com`
+ 
_The password to log in will be provided by the lab instructors._

+ 
----
$ ssh lab-user@workstation-<YOUR-GUID>.rhpds.opentlc.com
----

. Become `root` using sudo:
+
----
$ sudo -i
----

. Check the status of the whole environment, from the `workstation`, using ansible:
+
----
# ansible all -m ping -o
----
+
This establishes a connection to each of the machines in the environment to check if it is reachable.
In case the machines are up and running a success message for each host in the list will be displayed.
This is an example of a success message for the VM `cf.example.com`:
+
----
cf.example.com | SUCCESS => {"ansible_facts": {"discovered_interpreter_python": "/usr/libexec/platform-python"}, "changed": false, "ping": "pong"}
----
+
To check only the infrastructure machines the following command can be also used:
+
----
# ansible infra -m ping -o
----
+
[NOTE] 
As this environment is quite big, and it is generated and powered up for you in a cloud environment, some resources may suffer from issues or delays depending on the status of the cloud. Please review everything is running before proceeding forward. If you run into any issues, please reach out to lab instructors, they will be happy to help!     

=== Prepare to manage the environment. 
From a web browser, open each of the URLs below in its own window or tab, using these credentials (except when noted differently):

* *Username*: `admin`
* *Password*: `<to_be_provided>`

[cols="1,1,1",options="header"]
|=======
|Server| Internal Hostname (with proxy) | Public Hostname
|CloudForms|cf.example.com |cf-<YOUR-GUID>.rhpds.opentlc.com 
|Satellite 6|satellite.example.com | satellite-<YOUR-GUID>.rhdps.opentlc.com
|RHV Manager|rhvm.example.com| Not accessible without proxy 
|vCenter| vcenter.example.com| Not accessible without proxy
|=======

[NOTE]
You must accept all of the self-signed SSL certificates.

=== *Red Hat Virtualization Manager:* 

Address of WEB UI: `*\https://rhvm.example.com*`
 +
 Click on `Administration Portal` and use the following credentials:

* username: admin
* password: <to be provided>

In order to access the RHV Manager, you *have to use proxy*. If you did not configure proxy, please do so now. If you do not intend to use the proxy, this is the part of the lab which you would need to skip. In that case go to section <<CloudForms,4.4 CloudForms>>

.. Navigate to and click *Administration Portal* and log in using the provided credentials. Leave `Profile` field as `internal`.
+
image::rhv_login.png[RHV Login]

.. Verify that the Cluster is up and Hypervisors are active
+
image::rhv_dashboard.png[RHV Hypervisors up]

=== *VMware vCenter:* 
Address of WEB UI: `*\https://vcenter.example.com*`

Use the following credentials:

- username: `administrator@vsphere.local`
- password: <to be provided>

In order to access the vCenter, you *have to use proxy*. If you did not configure proxy, please do so now. If you do not intend to use the proxy, this is the part of the lab which you would need to skip. In that case go to section <<CloudForms,4.4 CloudForms>>

.. Click on *LAUNCH VSPHERE CLIENT (HTML5)* to get to the login screen
+
image::vsphere_web_client_0.png[vCenter Login]
+
[WARNING]
Use `administrator@vsphere.local` as the username to log in to vCenter.

After logging in you may be presented with a couple of warnings from vCenter, but those are caused by the cloud environment in which the lab is running.
They can be safely ignored or acknowledged in the WEB UI. 

.. Click *Click on VMs*.
+
image::vsphere_web_client_2.png[vCenter]

.. Verify that the 3 VMs: `hana`, `oracledb` and `tomcat` are running.
+
image::vsphere_web_client_3.png[vCenter]

[[CloudForms]]
=== *CloudForms:* 
Address of WEB UI: `cf.example.com` or `*cf-<YOUR-GUID>.rhpds.opentlc.com*` (without proxy)

Use the following credentials:

- username : admin
- password: <to be provided>

When you open one of the URLs stated above, you will be presented with CloudForms login screen:

image::cloudforms_login.png[CloudForms Login]

After logging in you will be presented with the CloudForms Dashboard.

image::cloudforms_dashboard.png[CloudForms Dashboard]

[NOTE]
When using the CloudForms interface, try to *_avoid using the Back_* button in your browser. It can lead you to a page you did not expect. 
Using the navigation bar on the left, or breadcrumb navigation on the top is always a better choice. 

=== Validate the Current Infrastructure in CloudForms

. Log in with user `admin` and the provided password in CloudForms. Once in the web interface, go to *Compute -> Infrastructure -> Providers*.
+
image::cloudforms_infrastructure_providers_1.png[CloudForms Infrastructure Providers 1]

. You should see a green tick mark in the provider boxes as shown in the screenshot below. 
If you by any chance, see an exclamation mark (*!*), or a cross ([red]#*x*#) in a provider, tick the provider's selection box, then go to *Authentication -> Re-check Authentication Status*.
+
image::cloudforms_infrastructure_providers_2.png[CloudForms Infrastructure Providers 2]

. To have proper information on all the resources available, tick both the provider's box, then go to *Configuration -> Refresh Relationships and Power States*.
+
image::cloudforms_infrastructure_providers_4.png[CloudForms Infrastructure Providers 4]
+

. Go to *Compute -> Infrastructure -> Virtual Machines*.
+
image::cloudforms_vms_1.png[CloudForms Virtual Machines 1]

. All VMs and Templates in both RHV and vSphere show as entities in CloudForms.
We can currently see the VMs deployed and running, as well as the ones which are powered off in our environment. 
+
image::cloudforms_vms_2.png[CloudForms Virtual Machines 2]
+
[NOTE]
If you had to initiate the re-validation of the providers in previous steps, you may have to wait a few minutes and refresh the screen before the VMs show up.

== Create an Infrastructure Mapping (vSphere to RHV)

. Navigate to the *Migration -> Infrastructure Mappings*.
+
image::infrastructure_mapping_0.png[Infrastructure Mapping 0]

. Click on *Create Infrastructure Mapping*.
+
image::infrastructure_mapping_1.png[Infrastructure Mapping 1]

. In the *step 1* of the wizard, *General*, type the name `VMware to RHV`, make sure that *Target Provider* is `Red Hat Virtualization` and click *Next*.
+
* A description may be added to make it easy to later on recognize the usage of the mapping.
+
image::infrastructure_mapping_2.png[Infrastructure Mapping 2]

. In the *step 2* of the wizard, *Map Compute*, select *Source Provider \ Datacenter \ Cluster* as `vSphere\Datacenter\VMCluster` and *Target Provider \ Datacenter \ Cluster* as `RHV\CoolDataCenter\TrustedCluster` and click *Add Mapping*, then click *Next*.
+
image::infrastructure_mapping_3.png[Infrastructure Mapping 3]

. In the *step 3* of the wizard, *Map Storage*, and having selected *Cluster01 (TrustedCluster)* as the cluster to work with, select *Source Provider \ Datacenter \ Datastore* as `vSphere\Datacenter\NFS-Storage` and *Target Datastores* as `RHV\VMStorageNFS` and click *Add Mapping*, then click *Next*.
+
image::infrastructure_mapping_4.png[Infrastructure Mapping 4]

. In the *step 4* of the wizard, *Map Networks*, *Cluster01 (TrustedCluster)* will be selected as the cluster to work with. 
We will start by mapping the network used by VMs to connect to each other. This describes which source networks on VMware map to the destination
network after the migration to RHV. 
We select *Source Provider \ Datacenter \ Network* as `vSphere \ Datacenter \ Net-Service` and *Target Network* as `RHV\service` and click *Add Mapping*.
+
image::infrastructure_mapping_5a.png[Infrastructure Mapping 5]
+
Do *not* click *Create* yet. 
+
We will continue by mapping the network used by VMs to expose services to the internet. We select *Source Provider \ Datacenter \ Network* as `vSphere\Datacenter\Net-Service-DMZ` and *Target Network* as `RHV\service-dmz` and click *Add Mapping*.
+
image::infrastructure_mapping_5b.png[Infrastructure Mapping 6]
+
And finally we can map the management network. To do so, select *Source Provider \ Datacenter \ Network* as `vSphere\Datacenter\Net-Management` and *Target Network* as `RHV\ovirtmgmt` and click *Add Mapping*, then click *Create*.
+
image::infrastructure_mapping_5c.png[Infrastructure Mapping 6]
+
The final Network Mapping should look like the following screenshot:
+
image::infrastructure_mapping_6.png[Infrastructure Mapping Network]

. Now you can click Create.
+
In the *step 5* of the wizard, *Results*, a message `All mappings in VMware to RHV have been mapped.` will appear. 
The only thing left to do is to click on *Close* on the last page of the wizard.
After the wizard closes, you will be presented with a finished mapping, as shown in the next screenshot.
+
image::infrastructure_mapping_final.png[Infrastructure Mapping Final]

In these steps an *Infrastructure Mapping* has been created in order to logically connect source and target resources using the data collected by Red hat CloudForms from both VMware vSphere and Red Hat Virtualization.

== Migrating VMs to RHV with a Migration Plan

Now that we have the Infrastructure mapped for both source and the destination clusters, we can get to the core of the matter. 
Creating a Migration Plan will enable us to choose which VMs we would like to migrate. There can be many plans created depending on internal or logical system separation. 
Maybe some systems cannot be migrated before a pre-requisite systems have been moved as well. 
The choice and planning of the migration is at the system administrator discretion. + 
So let's get on with it!

=== Create the migration plan

. Start in the CloudForms page accessed by navigating to *Migration -> Migration Plans*.
+
image::migration_plan_0.png[Migration Plan 0]

. Click on *Create Migration Plan*.

. In the *step 1* of the wizard, *General*, select in the drop down menu the *Infrastructure Mapping* to be used, `VMware to RHV`, add the name `Summit 2020 Lab` and click *Next*.
+
image::migration_plan_1.png[Migration Plan 2]
+
[NOTE]
Keeping the default option for *Select VMs* will take us to the VM menu selector. In this step we will be migrating the `oracledb`, `hana` and `tomcat` virtual machines.
For massive conversions, there is an option to use a CSV file upload, which is a better option in those cases.

. In the *step 2* of the wizard, *VMs*, we will choose the 3 VMs to be migrated to RHV. 
Please select, *oracledb*, *hana* and *tomcat* virtual machines to be migrated.
+
image::migration_plan_2.png[Migration Plan 3]

. In the *step 3* of the wizard, *Advanced Options*, we can assign *Pre* and *Post* migration playbooks to be executed during the migration. 
Since we have a couple of servers which are running distributions we would like to convert to RHEL during the conversion, we will enable *post* playbooks for them. 
Click on a *Select postmigration playbook service* drop-down and choose `PostMigration - Convert2RHEL`
+
image::migration_plan_3.png[Migration Plan 4]
+
In the same step make sure we select the VMs that need to be converted. Those are `oracledb` and `tomcat` (currently running Oracle Linux and CentOS, respectively).

. In the *step 4* of the wizard, *Schedule*, select *Start migration immediately* and click *Create*.
The wizard will close and the migration of the VMs will start immediately. 
+
image::migration_plan_4.png[Migration Plan 4]
+
[NOTE]
The migration plan can be scheduled to be ran at a later time, by choosing the other option.

. In the *step 5* of the wizard, *Results*, the message `Migration Plan: 'Summit 2020 Lab' is in progress` will be displayed. Click *Close*.
+
image::migration_plan_5.png[Migration Plan 5]

=== Monitor the Migration

. After you click the close button, you are be presented with a page showing the migration plans In Progress. It may take up to a minute for the progress box to start updating.
+
image::migration_running_1.png[Migration Running 3]

. Now the migration is executing. It takes some time for the pre-migration steps to be finished and the conversion process to start.
If we wished to, we could see the orchestration process in CloudForms logs.
From the workstation terminal you can SSH into CloudForms and tail the logs at on cf.example.com in the following directory: `/var/www/miq/vmdb/log/automation.log`. 
+
You can tail this file with `tail -f /var/www/miq/vmdb/log/automation.log`.
 +
Word of caution: the `automation.log` is storing a lot of logs and can present a huge amount of scrolling text. 
+
----
# ssh cf.example.com
# tail -f /var/www/miq/vmdb/log/automation.log
----
+
Once the pre-migration steps are finished and the conversion starts, each VM conversion process can be tracked in the Conversion Host.
Our conversion hosts are kvm1 and kvm2. So, for example we could do:
+
----
# ssh kvm1.example.com
# [root@kvm1 ~]# tail -f /var/log/vdsm/import/v2v-import-*.log
----

. CloudForms Migration interface shows migration status too.
Clicking on the running plan info box with the name `Summit 2020 Lab` will display the detailed info of the status. The screenshot below shows migration progress after 
some amount of time and might differ from what you currently see on the screen:
+
image::migration_running_2.png[Migration Running 2]

. As far as migration goes, `hana` is the only one which will not need to be converted. This means that it will get migrated and be running in the destination as first.
The other two machines, `tomcat` and `oracledb` will need more time, because after converting the disk to RHV, the OS still needs some time for the conversion to RHEL. 
The longest migration time is needed by the most complex VM deployment, and that is OracleDB running on Oracle Linux. + 
The total migration time is approximately `55-60` minutes, depending on the load on the cloud servers supporting the environment at the moment.
A thing to note is that the VMs being migrated are powered off during the migration process. + 
This can be seen if you navigate to *Compute -> Infrastructure -> Virtual Machines* in CloudForms. You may see double entries in the CloudForms UI, but those are both migrated and source machines presented on the different providers. They can be differentiated by the icon in the lower left corner of each VM.
+
image::migration_running_3.png[Migration Running 3]
+
You can return to the Migration Progress by clicking *Compute -> Migration -> Migration Plans*. Click on the running plan to get back to the details.
. After about `30` minutes, all of the VMs have finished with migration and those that need to be converted are running playbooks which will execute the conversion to RHEL. 
At this point it should already be visible in the list of VMs in RHV Web UI, as shown in the previous screenshot. 
[NOTE]
The following step can be only be accessed with the proxy enabled in the browser
+
Switch to RHV Manager tab in your browser and click on *Compute -> Virtual Machines*.  All the VMs will be now visible in the WEB UI of the Red Hat Virtualization as being powered up or already running. If by any chance some of the VMs are shown as down, and the playbook is running, it may be in the process of creating a snapshot or rebooting during the conversion. 
+
image::migration_running_rhv.png[Migration Running 9]
. Co back to the CloudForms tab. It takes additional `15-20` minutes in this environment for the playbooks to complete on the VMs being converted. 
CloudForms is now showing us that the migration has been completed successfully. 
The final view of the Migration Page should look something like this:
+
image::migration_running_finish.png[Migration Finished]

. Let's check if the VMs are up and running. Go to workstation VM and execute the following command:
+
----
# [root@workstation-repl ~]# ansible apps -m ping -o

oracledb.example.com | SUCCESS => {"ansible_facts": {"discovered_interpreter_python": "/usr/bin/python"}, "changed": false, "ping": "pong"}
hana.example.com | SUCCESS => {"ansible_facts": {"discovered_interpreter_python": "/usr/bin/python"}, "changed": false, "ping": "pong"}
tomcat.example.com | SUCCESS => {"ansible_facts": {"discovered_interpreter_python": "/usr/bin/python"}, "changed": false, "ping": "pong"}

[root@workstation-repl ~]#
----

. We can see that all of the VMs are operational and accessible. We can do a migration review within CloudForms as well
If you go to the main Migration Plans page of CloudForms you should see something similar to the following screenshot:
+
image::migration_finished_1.png[Migration Finished 1]
+
Here you can see that the migration has completed successfully, along with the details about INfrastructure Mapping used and total time for the migration.

. Additionally the migration logs can be downloaded and accessed post VM migration. This is useful for troubleshooting errors or just to check the migration details. It's worth mentioning that if the migration fails prior to the start of VM disk conversion, this log will not be available. The logs are in plain text format and can be quite large, so the download can take some time. +
To access the log just click on the plan we just migrated, choose a VM from the list and click **Download Log**. From the drop-down you can choose which log you would like to download.
+
image::migration_log.png[Migration Log Access]

== Insights and Remediation

Now that we have all of our VMs converted and automatically registered to a local Satellite server, we can use the insights to examine the systems.
The Insights dashboard, Inventory and Planner will help us discover, analyze and fix the problems found in the environment. 

=== Logging into Satellite and checking status
. First, we have to log in to our Satellite 6 server. 
The host address is `satellite.example.com`, the username is `admin` and password is the one provided by lab presenters.

. After you log in you will be taken to the Satellite Overview. 
At the left navigation menu, close to the bottom there will be an Insights link. Click on it to be taken to Insights details page. 
All of the systems listed there have been subscribed to this Satellite and have `insights-client` installed on them. 

image::satellite_menu_insights.png[Satellite Insights]

. After clicking on the menu, you will be taken to Insights Overview. Here you can see that all the hosts we migrated are listed there. 
And to the right side of it, there are some actions we could take to make the systems more reliable and stable. By using a proactive remediation we can fix 
the flaws in the systems' configuration which could potentially lead to outages, downtime or security issues. 
+
image::insights_dash.png[Insights Dashboard]
+
Now that we know that there are things to be improved in our environment, we can go through the next steps. 
We will see how we can use Satellite, Insights and Ansible to quickly fix the issues detected. 

=== Insights Inventory and Planner

. On the left menu, hover over Insights and then click on Inventory. 
+
image::insights_inv.png[Insights Inventory]
+
. This will present an inventory page which allows us to check the issues Insights has marked detected as possible problems.
In the table we can see the list of hostnames, along with number of actions Insights is capable of executing. 
+
image::insights_inv2.png[Insights Inventory2]
+
. You can use the links in that table, either hostnames or number of actions to explore the suggested actions by Insights. 
We can use `tomcat` to bring up an example of this. Click on the hostname and it should pop up a window with a more detailed description of the action. 
+
image::insights_inv3.png[]

. This is a simple enough fix, which can be done manually. But what we will do is create plans which can include multiple fixes, and run them to let Insights and Ansible fix the selected issues for us. 
For this - we need a plan!

And plan is what we are going to make. Well, plans rather since we will fix systems belonging to different groups separately. 
`tomcat` as our _application server_ will be fixed separately, and our database `oracledb` will be fixed as an _infrastructure server_ remediation with a separate plan.
There are many ways to do this, so we will pick one for you and explain the steps:

. Click on X to close the overview of the fix, if you hadn't done so already. Since we mentioned we needed a plan, we can go directly to Planner to help us create some. 
On the left hand menu, hover over Insights and click on Planner.
+
image::insights_planner.png[]
+
. An empty page will load since there are no plans created yet. We will fix that momentarily. Click on the *Create a Plan* in the upper right corner:
+
image::insights_planner2.png[]
+
. A dialog will appear which will as us to provide some information regarding the remediation plan we are creating. *Create new plan* is selected by default, so we will leave it that way. 
For the first plan we will set the plan name as `tomcat`, and make sure we click on a radio button to select *Specific System*, and choose `tomcat.example.com` from the drop-down menu.
+
image::insights_planner3.png[]
+
. Select the checkbox with actions you want to execute, or if you wish all of the actions to be included in the plan select the top left box.
We are going to use all of the actions available, so please click the checkbox in the top left of the table. 
After that, go ahead and click *Save*.
+
image::insights_planner4.png[]
+
. To minimize the plan window click on the small arrows in the upper right corner. 
+
image::insights_planner5.png[]
+
We are done, we can continue creating the rest of the plans in the same manner. 

Let's create another plan for `oracledb` remediation. 
The steps are the same as what we just did for `tomcat`, but this time we do it for `oracledb`:

.. Click on *Create a plan*
.. Give it a name, say "OracleDB"
.. Click on radio-button for *Specific System*
.. Choose `oracledb.example.com`
.. Click on checkbox in the top of the table of Actions, this should select all and 
.. Click *Save*
.. Minimize with little arrow icon on the upper right. 

When done, you should have a page looking like the following screenshot. Both plans are available and ready to be executed. 

image::insights_planner5.png[]

=== Running remediation and confirming fixes

Now that we have plans in place, we can go into each of those and let it execute its automatic remediation playbooks. 
We can start with OracleDB. 

. Click on the box with OracleDB and you will be presented with the final view of the plan we made. 
You could use a little pencil in the top corner to enter the edit mode and modify the plan when needed.
However, we do not have any other actions to assign to this system, so we can go ahead and click on *Run Playbook* at the bottom.
+
image::run_playbooks.png[]
+
After a couple of seconds a new screen will appear showing the progress of the remediation.
+
image::run_playbooks2.png[]
+
. The playbook should complete in less then a minute, and will present a screen with a report. 
+
image::run_playbooks3.png[]
+
If you wish you can examine the playbook run by scrolling to the bottom and selecting *Host task* from the drop-down menu of Actions.
+
image::run_playbooks4.png[]
+
. If you do, do not forget to come back to planner, by clicking on *Insights -> Planner*. We have one more plan to execute. 
+
Our automated remediation fixed most of the issues in the performance on the `oracledb` host. 
There was one setting it could not change automatically, and that one is still being listed.
+
The good news is that it is not just being listed, it inludes the recommended manual steps required to correct the issue found.
We can see these steps if we click on the hostname either in Insights Overview or in Planner. 
+
. For `tomcat` we will repeat the steps used for running the remediation on `oracledb`
+
image::run_playbooks5.png[]

Let's repeat the steps for remediation of `tomcat`:

.. Click on `tomcat` plan
.. Scroll down to *Run Playbook*
.. Click on it and wait for the progress window to load


When finished the planner will show all of the issues for tomcat resolved, and no actions to take in the plan. 

image::run_finished.png[]


*CONGRATULATIONS* 

you have reached the end of this lab. 