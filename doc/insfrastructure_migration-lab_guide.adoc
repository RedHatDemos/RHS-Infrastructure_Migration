:scrollbar:
:data-uri:
:toc2:
:imagesdir: images

== Infrastructure Migration 1.1 beta - Migration Guide

:numbered:

== Overview

This Lab Environment uses Red Hat CloudForms, and the Infrastructure Migration plugin (a.k.a. v2v-plugin) included with it, as a broker to migrate VMs from VMware vSphere to Red Hat Virtualization (RHV) and Red Hat OpenStack Platform (OSP). 

The password to access all services is available link:https://mojo.redhat.com/docs/DOC-1174612-accessing-red-hat-solutions-lab-in-rhpds[here]

In this environment the following configuration steps are already done:

* Configure the conversion hosts in the target providers (RHV and OSP)
* Provide credentials for virtualization providers and conversion hosts in CloudForms
* Assign tags for the conversion hosts in CloudForms
* Configured Ansible hosts file in `workstation` and ssh keys exchanged (`/root/.ssh/id_rsa`)

The providers are:

* Source - VMware vSphere
* Targets - Red Hat Virtualization and Red Hat OpenStack Platform

After this initial step, the resource equivalence between the two providers, involved in each migration, has to be defined in an *Infrastucture Mapping* including:

* Clusters
* Storage Domains
* Networks

To perform the migration one *Conversion Host* has been configured for each target provider (RHV / OSP). 

* In the RHV case the one of the RHEL hypervisors is configured as conversion host
* In the OSP case an instance running in the `conversion` project is configured as conversion host

Finally, a *Migration Plan* has to be executed to perform the final migration.

.Goal
* Migrate several VMs from VMware vSphere to Red Hat Virtualization and Red Hat OpenStack Platform, using Red Hat CloudForms as a broker, with the tooling included in the Red Hat Infrastructure Migration Solution
+
NOTE: The source VMs are not deleted, only powered off once the migration is done. This measure can also be used as "rollback" in case a migration failure occurs.

Required versions of products used:

[cols="1,1",options="header"]
|=======
|Product |Version
|CloudForms |4.7.0+ 
|Red Hat Virtualization |4.2.5+
|Red Hat OpenStack Platform |13+
|Red Hat Enterprise Linux (Conversion Host) |7.6+
|Conversion Host Image |1.7+
|VMware vSphere |5.5+
|=======

== Requirements to access and perform this lab

=== Base requirements

* A computer with access to Internet :-)
* Adobe Flash 15 or higher must be enabled in Firefox or Chromium used for vCenter connectivity
* SSH client (for Microsoft Windows users link:https://www.putty.org/[Putty] is recommended)
* Firefox 17 or higher, or Chromium / Chrome
+
[NOTE]
Grammarly plugin for Chrome causes problems when managing CloudForms. Please deactivate it while doing this lab.

=== Obtaining or enabling access credentials

. First time login?, forgot login or password? Go to https://www.opentlc.com/account 

. Note, your username should NOT have an *@* in it. 

. Red Hat Partners can also request access to the Red Hat Product Demo System (link:https://rhpds.redhat.com[RHPDS]) by sending an email to open-program@redhat.com. 

. Passwords to the services is referred as `<to_be_provided>`. The credentials to access all services are available link:https://mojo.redhat.com/docs/DOC-1174612-accessing-red-hat-solutions-lab-in-rhpds[here]. If you can't access it please contact GPTE or the link:https://mojo.redhat.com/community/marketing/vertical-marketing/horizontal-solutions/people[Horizontal Solutions Team].

== Preparation

=== Provision Your Migration Environment

. Log in to the link:https://rhpds.redhat.com/[Red Hat Product Demo System] with your provided credentials. 
+
image::rhpds_login.png[RHPDS]

[start=2]
. Go to *Services -> Catalogs*.
. Under *All Services -> Red Hat Solutions*, select *Infrastructure Migration 1.1 Alpha*.
. On the right pane, click *Order*.
+
image::rhpds_catalog.png[RHPDS]

[start=5]
. Please, read carefully all of the information on the resulting page, check the box to confirm you understood the runtime warning message, and then click *Submit*.
+
image::rhpds_order.png[RHPDS]

[IMPORTANT]
====
* It takes about 20 ~ 25 minutes for the demo to load completely and become accessible.
** Wait for the full demo to load, even if some of its systems are marked "Up."
* Watch for an email with information about how to access your demo environment.
** Make note of the email's contents: a list of hostnames, IP addresses, and your GUID.
** Whenever you see <YOUR-GUID> in the demo instructions, replace it with the GUID provided in the email.
* You can get real-time updates and status of your demo environment at https://www.opentlc.com/rhpds-status.
====

[TIP]
Be mindful of the runtime of your demo environment! It may take several hours to complete the demo, so you may need to extend the runtime. This is especially important in later steps when you are building virtual machines. For information on how to extend runtime and lifetime, see https://www.opentlc.com/lifecycle.

== Environment

A full new migration environment is deployed on every request. To make the environment unique a 4 character identifier is assigned to it (i.e. `1e37`), this identifier is referred in this documentation as *YOUR-GUID*.  

The migration environment consists of the following systems:

image::blueprint.png[Blueprint]

[cols="1,1,1,2",options="header"]
|=======
| Hostname | Internal IP | External name | Description
|`workstation.example.com` |`192.168.0.10` | workstation-<YOUR-GUID>.rhpds.opentlc.com |Jump host and Ansible host
|`storage.example.com` |`192.168.0.254` | workstation-<YOUR-GUID>.rhpds.opentlc.com | NFS server
|`cf.example.com` |`192.168.0.100` |  cf-<YOUR-GUID>.rhpds.opentlc.com |CloudForms server
|`rhvm.example.com` |`192.168.0.35` | rhvm-<YOUR-GUID>.rhpds.opentlc.com |Red Hat Virtualization Manager server
|`kvm1.example.com` |`192.168.0.41` | kvm1-<YOUR-GUID>.rhpds.opentlc.com |KVM hypervisor managed by Red Hat Virtualization
|`kvm2.example.com` |`192.168.0.42` | kvm2-<YOUR-GUID>.rhpds.opentlc.com |KVM hypervisor managed by Red Hat Virtualization
|`horizon.example.com` |`192.168.10.19` | horizon-<YOUR-GUID>.rhpds.opentlc.com |Red Hat OpenStack Platform web UI and User API Endpoint
|`controller.example.com` |`10.100.0.111` | controller-<YOUR-GUID>.rhpds.opentlc.com |Red Hat OpenStack Platform controller
|`compute0.example.com` |`10.100.0.105` | compute0-<YOUR-GUID>.rhpds.opentlc.com |Red Hat OpenStack Platform compute
|`compute1.example.com` |`10.100.0.107` | compute1-<YOUR-GUID>.rhpds.opentlc.com |Red Hat OpenStack Platform compute
|`rhvm.example.com` |`192.168.0.35` | rhvm-<YOUR-GUID>.rhpds.opentlc.com |Red Hat Virtualization Manager server
|`esx1.example.com` |`192.168.0.51` | N/A |ESXi hypervisor
|`esx2.example.com` |`192.168.0.52` | N/A |ESXi hypervisor
|`vcenter.example.com` |`192.168.0.50` | vcenter-<YOUR-GUID>.rhpds.opentlc.com |VMware vCenter server
|=======

The architecture of the deployment can be depicted as it follows:

image::architecture_diagram.png[Architecture Diagram]

* Networks
Networks used in the environment

[cols="1,1,2",options="header"]
|=======
| Network Name | IP range | Description
| `Admin` | `192.168.x.x/16` | General adminitration and Storage network.
| `Service` | `10.10.0.x/24` | Internal network for the app to connect LB to EAP and to DB. 
| `Servicer-DMZ` | `10.9.0.x/24` | External DMZ network to publish the app. Also access to the user API for OSP and Horizon (Provider network)
| `OSP Provisioning` | `10.100.0.x/24` | OpenStack provisioning network (includes Director and PXE), as well as access to the Admin API endpoint and control plane.  
|=======

* Virtual Machines 
This deployment of the migration environment includes the following VMs provisioned in the vSphere environment in order to be migrated:

[cols="1,1,2",options="header"]
|=======
| Name | IPs | Description
| `jboss0.example.com` | 10.10.0.110 | Red Hat Enterprise Linux 7 host running JBoss EAP, connected to the `Service` network.
| `jboss1.example.com` | 10.10.0.111 | Red Hat Enterprise Linux 7 host running JBoss EAP, connected to the `Service` network.
| `lb.example.com` | 10.10.0.100 , 10.9.0.100 | Red Hat Enterprise Linux 7 host running JBoss Core Service Apache HTTP server configured with mod_cluster to proxy traffic to `jboss0` and `jboss1`, connected to the `Service` and `Servicer-DMZ` networks.
| `db.example.com` | 10.10.0.120 | Red Hat Enterprise Linux 7 host running PostgreSQL providing service to `jboss0` and `jboss1` through the `Service` network.
|=======

* An external service is configured as https://app-<YOUR-GUID>.rhpds.opentlc.com pointing to the Load Balancer to make the Ticket Monster app accesible.

== Getting Started

. Once the system is running, use SSH to access your demo server using your OPENTLC login name and private SSH key.

* Using a Unix/Linux system:
+
----
$ ssh -i /path/to/private_key <YOUR-OpenTLC-USERNAME-redhat.com>@workstation-<YOUR-GUID>.rhpds.opentlc.com
----

* Example for user 'batman' and GUID '1e37', using the default ssh private key:
+
----
$ ssh -i ~/.ssh/id_rsa batman-redhat.com@workstation-1e37.rhpds.opentlc.com
----

. Become `root` using the provided password:
+
----
$ sudo -i
----

. Check the status of the whole environment, from the `workstation`, using ansible:
+
----
# ansible all -m ping
----
+
This command establishes a connection to all the machines in the environment (except ESXi servers). 
In case the machines are up an running a success message, per each, will show up. 
This is an example of a success message for the VM `cf.example.com`:
+
----
cf.example.com | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
----
+ 
To check the infrastructure machines the following command can be also used:
+
----
# ansible infra -m ping
----
There are 4 VMs in the vCenter environment hosting an app with JBoss Core Services' Apache HTTP + modcluster as loadbalancer, two JBoss EAP 7 in domain mode, and a Postgresql database.
To check only if these ones are running, you may use the following command:
+
----
# ansible app -m ping
----
+ 
[NOTE]
As this environment is quite big, and it is generated and powered up for you on a cloud environment, some resources may suffer from issues or delays depending on the status of the cloud. You may need to manually start up or reboot some of them. Please review everything is running before proceeding forward.

. Establish an SSH connection to the CloudForms server and monitor `automation.log`:
+
----
# ssh cf.example.com
# tail -f /var/www/miq/vmdb/log/automation.log
----
+
[TIP]
The log entries are very long, so it helps if you stretch this window as wide as possible.
+
[NOTE]
The log entries can be also seen in the CloudForms web UI in *Automation -> Automate -> Log*.

. Verify that the Ticket Monster app is running:

* Point your browser to https://app-<YOUR-GUID>.rhpds.opentlc.com and check it is running:
+
image::app-ticketmonster-running.png[Ticket Monster app running]
[NOTE]
You must accept all of the self-signed SSL certificates.
+
image::ssl_cert_warning.png[SSL Cert Warning]
+
If the ticketmonster app is not running, you may run the following command in `workstation`:
+
----
# start_vms
----

. Prepare to manage the environment. From a web browser, open each of the URLs below in its own window or tab, using these credentials (except when noted):

* *Username*: `admin`
* *Password*: `<to_be_provided>`
+
[NOTE]
You must accept all of the self-signed SSL certificates.

* *Red Hat Virtualization Manager:* `https://rhvm-<YOUR-GUID>.rhpds.opentlc.com`
.. Navigate to and click *Administration Portal* and log in using `admin`, `<to_be_provided>`, and `internal`.
+
image::rhv_login.png[RHV Login]

.. Verify that the Cluster is up and Hypervisors are active
+
[TIP]
As this is nested virtualization, sometimes the CPU type of the hypervisor is changed. 
+
image::rhv_hypervisors_up.png[RHV Hypervisors up]

* *vCenter:* `https://vcenter-<YOUR-GUID>.rhpds.opentlc.com`
+
image::vsphere_web_client_0.png[vCenter Login]

** Flash Player is required.
+
[TIP]
Modern browsers have flash player disabled by default. You  need to enable it to access and use the web manager interface in vSphere 5.5.

* Click on *Log in to vSphere Web Client*
+
image::vsphere_web_client_1.png[vSphere Web Client Login]
+
[WARNING]
Use `root` as the username to log in to vCenter.

* Click *Click on VMs and Templates*.
+
image::vsphere_web_client_2.png[vCenter]

.. Click *VMs and Templates* and verify that the 4 VMs `lb.example.com`, `jboss0.example.com`, `jboss1.example.com` and `db.example.com` are running. 
+
image::vsphere_web_client_3.png[vCenter]

* *CloudForms:* `https://cf-<YOUR-GUID>.rhpds.opentlc.com`
+
image::cloudforms_login.png[CloudForms Login]
+
[TIP]
You can also find these URLs in the email provided when you provisioned the demo environment.
+
image::cloudforms_dashboard.png[CloudForms Dashboard]


* *OpenStack Horizon:* `http://horizon-<YOUR-GUID>.rhpds.opentlc.com`
+
image::horizon_login.png[Horizon Login]
+
[NOTE]
Horizon access still does not have https configured (sorry!)
+
image::horizon_dashboard.png[Horizon Dashboard]

=== Validate the Current Infrastructure in CloudForms

. Log in with user `admin` and the provided password in CloudForms. Once in the web interface, go to *Compute -> Infrastructure -> Providers*.
+
image::cloudforms_infrastructure_providers_1.png[CloudForms Infrastructure Providers 1]

. If you see an exclamation mark (*!*), or a cross (*x*) in a provider, check the provider's box, go to *Authentication -> Re-check Authentication Status*.
+
image::cloudforms_infrastructure_providers_2.png[CloudForms Infrastructure Providers 2]
+
image::cloudforms_infrastructure_providers_3.png[CloudForms Infrastructure Providers 3]
+
[TIP]
Take into account that vCenter may take longer to start.

. To have proper information on all the resources available, check the provider's box, go to *Configuration -> Refresh Relationships and Power States*.
+
image::cloudforms_infrastructure_providers_4.png[CloudForms Infrastructure Providers 4]
+
image::cloudforms_infrastructure_providers_5.png[CloudForms Infrastructure Providers 5]

. Time to go to *Compute -> Clouds -> Providers*.
+
image::cloudforms_cloud_providers_1.png[CloudForms Clouds Providers 1]

. If you see an exclamation mark (*!*), or a cross (*x*) in the provider, check the provider's box, go to *Authentication -> Re-check Authentication Status*.
+
image::cloudforms_cloud_providers_2.png[CloudForms Clouds Providers 2]
+
image::cloudforms_cloud_providers_3.png[CloudForms Clouds Providers 3]

. To have proper information on all the resources available, check the provider's box, go to *Configuration -> Refresh Relationships and Power States*.
+
image::cloudforms_cloud_providers_4.png[CloudForms Clouds Providers 4]
+
image::cloudforms_cloud_providers_5.png[CloudForms Clouds Providers 5]

. Go to *Compute -> Infrastructure -> Virtual Machines -> VMs -> All VMs*.
+
image::cloudforms_vms_1.png[CloudForms Virtual Machines 1]

. All VMs, Orphaned VMs and Templates in both RHV and vSphere show as entities in CloudForms.
+
image::cloudforms_vms_2.png[CloudForms Virtual Machines 2]
+
[NOTE]
If you needed to validate providers, you may have to wait a few minutes and refresh the screen before the VMs show up.

. Select the pane *VMs & Templates* and, in it, the *vSphere* provider.

. Only the VMs and Templates in vSphere will show.
+
image::cloudforms_vms_3.png[CloudForms Virtual Machines 3]
+
[TIP]
This is a good way to check that the app VMs are up and running and start the sopped ones.

=== Check Conversion Hosts

*Conversion hosts* are the machines that will connect to the vCenter API, in the same fashion backup software would work, get the data from it, perfom the changes required, and send it to the RHV manager API or OSP User API Endopint.
 
[WARNING]
*Conversion hosts* used to be managed by using *tags* in *CloudForms* in Infrastructure Migration Solution version *1.0*. For version *1.1*, tags are deprecated and new *database entries* are being used to identify them. 
 
. Let's check the conversion host configuration by accesing the *rails console*

* First connect to *CloudForms* via SSH from the `workstation`
+
----
# ssh cf.example.com
----

* Then go to the rails app folder and start the rails console
+
----
[root@cf ~]# vmdb
[root@cf vmdb]# pwd
/var/www/miq/vmdb
[root@cf vmdb]# rails c
irb(main):001:0> 
----

* Let's get the list of the conversion hosts
+
----
[root@cf vmdb]# rails c
** CFME 5.10.0.29, codename: Hammer
Loading production environment (Rails 5.0.7.1)
irb(main):001:0> pp ConversionHost.all
PostgreSQLAdapter#log_after_checkout, connection_pool: size: 5, connections: 1, in use: 1, waiting_in_queue: 0
[#<ConversionHost:0x000000000affe560
  id: 1,
  name: "kvm1.example.com",
  address: nil,
  type: nil,
  resource_type: "Host",
  resource_id: 3,
  version: nil,
  max_concurrent_tasks: nil,
  vddk_transport_supported: true,
  ssh_transport_supported: false,
  created_at: Wed, 12 Dec 2018 16:17:17 UTC +00:00,
  updated_at: Wed, 12 Dec 2018 16:17:17 UTC +00:00,
  concurrent_transformation_limit: nil,
  cpu_limit: nil,
  memory_limit: nil,
  network_limit: nil,
  blockio_limit: nil>,
 #<ConversionHost:0x000000000affe2e0
  id: 2,
  name: "conversion2",
  address: nil,
  type: nil,
  resource_type: "VmOrTemplate",
  resource_id: 20,
  version: nil,
  max_concurrent_tasks: 5,
  vddk_transport_supported: true,
  ssh_transport_supported: nil,
  created_at: Tue, 18 Dec 2018 14:45:14 UTC +00:00,
  updated_at: Tue, 18 Dec 2018 14:45:43 UTC +00:00,
  concurrent_transformation_limit: nil,
  cpu_limit: nil,
  memory_limit: nil,
  network_limit: nil,
  blockio_limit: nil>]
=> #<ActiveRecord::Relation [#<ConversionHost id: 1, name: "kvm1.example.com", address: nil, type: nil, resource_type: "Host", resource_id: 3, version: nil, max_concurrent_tasks: nil, vddk_transport_supported: true, ssh_transport_supported: false, created_at: "2018-12-12 16:17:17", updated_at: "2018-12-12 16:17:17", concurrent_transformation_limit: nil, cpu_limit: nil, memory_limit: nil, network_limit: nil, blockio_limit: nil>, #<ConversionHost id: 2, name: "conversion2", address: nil, type: nil, resource_type: "VmOrTemplate", resource_id: 20, version: nil, max_concurrent_tasks: 5, vddk_transport_supported: true, ssh_transport_supported: nil, created_at: "2018-12-18 14:45:14", updated_at: "2018-12-18 14:45:43", concurrent_transformation_limit: nil, cpu_limit: nil, memory_limit: nil, network_limit: nil, blockio_limit: nil>]>

----
+
[NOTE]
This functionality is expected to be available through the web interface for version 1.2

* Let's get the info on the conversion host `kvm1.example.com`
+
----
irb(main):001:0> ConversionHost.find_by(name: 'kvm1.example.com')
PostgreSQLAdapter#log_after_checkout, connection_pool: size: 5, connections: 1, in use: 1, waiting_in_queue: 0
=> #<ConversionHost id: 1, name: "kvm1.example.com", address: nil, type: nil, resource_type: "Host", resource_id: 3, version: nil, max_concurrent_tasks: nil, vddk_transport_supported: true, ssh_transport_supported: false, created_at: "2018-12-12 16:17:17", updated_at: "2018-12-12 16:17:17", concurrent_transformation_limit: nil, cpu_limit: nil, memory_limit: nil, network_limit: nil, blockio_limit: nil>
----

* Let's get the info on the conversion host `conversion2`
+
----
irb(main):002:0> ConversionHost.find_by(name: 'conversion2')
=> #<ConversionHost id: 2, name: "conversion2", address: nil, type: nil, resource_type: "VmOrTemplate", resource_id: 20, version: nil, max_concurrent_tasks: 5, vddk_transport_supported: true, ssh_transport_supported: nil, created_at: "2018-12-18 14:45:14", updated_at: "2018-12-18 14:45:43", concurrent_transformation_limit: nil, cpu_limit: nil, memory_limit: nil, network_limit: nil, blockio_limit: nil>
----

=== Check Credentials

The credentials that CloudForms uses to connect to Conversion Hosts are stored in the *Host* properties or in the *Provider* properties

. Now let's check the credentials in the UI. On the `cf` system, go to *Compute -> Infrastructure -> Hosts*.
+
image::conversion_host_1.png[Conversion Host 1]

. Click *kvm1.example.com*.
+
image::conversion_host_2.png[Conversion Host 2]

. Select *Configuration -> Edit this item* (back in `kvm1.example.com`).
+
image::conversion_host_8a.png[Conversion Host 8]
+
image::conversion_host_8b.png[Conversion Host 8]

. Check that *Username* has `root` and Password is set. You can click *Validate* to verify they are OK then the message "Credential validation was successful" will appear. This is needed to be able to connect to the conversion host and initiate the conversion.
+
image::conversion_host_9.png[Conversion Host 9]

. The conversion host is ready.

. Time to check credentials in `OpenStack` *Cloud Provider*,  go to *Compute -> Clouds -> Providers*.
+
image::conversion_host_provider_1.png[Conversion Host Cloud Provider 1]

. Click *OpenStack*.
+
image::conversion_host_provider_2.png[Conversion Host Cloud Provider 2]

. Select *Configuration -> Edit this Cloud Provider* (while in `OpenStack` provider page).
+
image::conversion_host_provider_3.png[Conversion Host Cloud Provider 3]
+
image::conversion_host_provider_4.png[Conversion Host Cloud Provider 4]

. Check the *RSA key pair* tab in which the *Username* is `root` and the `Private Key` is already configured.
+
image::conversion_host_provider_5.png[Conversion Host Cloud Provider 5]

. The conversion credentials for the Cloud Provider are ready.

== Create an Infrastructure Mapping (vSphere to RHV)

. Navigate to the *Compute -> Migration -> Infrastructure Mappings*.
+
image::infrastructure_mapping_1.png[Infrastructure Mapping 1]

. Click on *Create Infrastructure Mapping*.
+
image::infrastructure_mapping_2.png[Infrastructure Mapping 2]

. In the *step 1* of the wizard, *General*, type the name `ticket-monster-map-rhv`, select as *Target Provider* `Red Hat Virtualization`  and click *next*.
+
* A description may be added to make it easy to, later on, recognize the usage of the mapping.
+
image::infrastructure_mapping_3.png[Infrastructure Mapping 3]

. In the *step 2* of the wizard, *Clusters*, select *Source Cluster* as `vSphere\DC01\Cluster01` and *Target Cluster* as `RHV\CoolDataCenter\TrustedCluster` and click *Add Mapping*, then click *next*.
+
image::infrastructure_mapping_4.png[Infrastructure Mapping 4]

. In the *step 3* of the wizard, *Datastores*, and having selected *Cluster01 (TrustedCluster)* as the cluster to work with, select *Source Datastore* as `vSphere\Datastore` and *Target Datastore* as `RHV\VMStorageNFS` and click *Add Mapping*, then click *next*.
+
image::infrastructure_mapping_5.png[Infrastructure Mapping 5]

. In the *step 4* of the wizard, *Networks*, and having selected *Cluster01 (TrustedCluster)* as the cluster to work with. We will start by mapping the netowrk used by VMs to connect yto each other (i.e. JBoss EAP to the Database). We select *Source Network* as `vSphere\App-Internal-DPortGroup` and *Target Network* as `RHV\service` and click *Add Mapping*.
+
image::infrastructure_mapping_6a.png[Infrastructure Mapping 6]
+
* We will continue by mapping the network used by VMs to expose services to the internet(i.e. the Load Balancer exposing the Ticket Monster app). We select *Source Network* as `vSphere\App-DMZ-DPortGroup` and *Target Network* as `RHV\service-dmz` and click *Add Mapping*.
+
image::infrastructure_mapping_6b.png[Infrastructure Mapping 6]
+
* Finally we canmap the management network. To do so, select *Source Network* as `vSphere\Management Network` and *Target Network* as `RHV\ovirtmgmt` and click *Add Mapping*, then click *create*.
+
image::infrastructure_mapping_6.png[Infrastructure Mapping 6]

. In the *step 5* of the wizard, *Results*, a message `All mappings in ticket-monster-map-rhv have been mapped.` shall appear. Click *close*.
+
image::infrastructure_mapping_7.png[Infrastructure Mapping 7]
+
image::infrastructure_mapping_8.png[Infrastructure Mapping 8]

In these steps an *Infrastructure Mapping* has been created in order to simplify source and target resources using the data collected by Red hat CloudForms from both VMware vSphere and Red Hat Virtualization.

== Migrating VMs to RHV with a Migration Plan

=== Create the migration plan

. Start in the CloudForms page accessed by navigating to *Compute -> Migration -> Migration Plans*.
+
image::migration_plan_0.png[Migration Plan 0]

. Click on *Create Migration Plan*.
+
image::migration_plan_1.png[Migration Plan 1]

. In the *step 1* of the wizard, *General*, select in the drop down menu the *Infrastructure Mapping* to be used, `ticket-monster-rhv`, add the name `ticket-monster-plan-app` and click *next*.
+
image::migration_plan_2.png[Migration Plan 2]
+
[NOTE]
Keeping the default option will take us to the VM menu selector. For massive conversions a CSV file upload can be the right choice.

. In the *step 2* of the wizard, *VMs*, select the *jboss0* and *jboss1*  virtual machines, as the ones to be migrated.
+
image::migration_plan_3.png[Migration Plan 3]
+
[NOTE]
VM selector has a filter to help find a set of VMs within a long list. We may try filtering by the term `jboss`. 

. In the *step 3* of the wizard, *Advanced Options*, we can assign *Pre* and *Post* migration playbooks to be executed during the migration. We won't use this feature just yet. Click *Next*
+
image::migration_plan_4.png[Migration Plan 4]

. In the *step 4* of the wizard, *Schedule*, select *Save migration plan to run later*. Click *Create*
+
image::migration_plan_5.png[Migration Plan 5]
+
[NOTE]
The migration plan can be run immediately, by choosing the other option. 

. In the *step 5* of the wizard, *Results*, the message `Migration Plan: ticket-monster-plan-app has been saved` shall appear. Click *Close*.
+
image::migration_plan_6.png[Migration Plan 6]

. Back to the migration page we will see how the *Infrastructure Mapping* and *Migration Plan* are ready to be run
+
image::migration_plan_7.png[Migration Plan 7]

=== Launch Migration

. To launch the migration, while in the *Compute -> Migration* page, click on the *Migrate* button in the *ticket-monster-plan-app*.
+
image::migration_running_1.png[Migration Running 1]

. The migration will get initiated. All data is gathered and preflight checks are executed.
+
image::migration_running_2.png[Migration Running 2]

. The plan gets auto-approved. Migration starts
+
image::migration_running_3.png[Migration Running 3]

. Now the migration is executing. We can see the orchestration process in Cloudforms logs
+
----
# ssh cf.example.com
# tail -f /var/www/miq/vmdb/log/automation.log
----
+
Once the pre-migration steps are finished and the conversion starts, each VM conversion process can be tracked in the Conversion Host:
+
----
# ssh kvm1.example.com
# tail -f /var/log/vdsm/import/v2v-import-*
----

. CloudForms Migration interface shows migration status too
+
image::migration_running_4.png[Migration Running 4]

. Clicking on the running plan info box will display the detailed info of the status
+
image::migration_running_5.png[Migration Running 5]

. Progress can be followed in this page
+
image::migration_running_6.png[Migration Running 6]

. For the time of the migration the JBoss EAP servers, `jboss0` and `jboss1` will be powered off in `vSphere`, migrated and then powered on in `RHV`.
+
image::migration_running_7.png[Migration Running 7]

. It is possible to check in *RHV* interface, in *Compute -> Virtual Machines* how the VM gets imported.
+
image::migration_running_8.png[Migration Running 8]

. Once the migration is finishing ...
+
image::migration_running_9.png[Migration Running 9]

. ... the VMs get powered up
+
image::migration_running_10.png[Migration Running 10]
+
image::migration_running_11.png[Migration Running 11]

. The migration gets completed.
+
image::migration_running_12.png[Migration Running 12]

. Let's check if the VMs are up and running using the following command:
+
----
# ansible app -m ping
----

. It's time to check the *app* running and accesible via the URL https://app-<YOUR-GUID>.rhpds.opentlc.com
+
image::migration_running_14.png[Migration Running 14]

. Migration can be reviewed in the Main Migration page in CloudForms
+
image::migration_running_15.png[Migration Running 15]

. Additionally the migration log can be downloaded and accessed post VM migration. This is useful for troubleshooint errors or just to check the migration details. It's worth nothing that if the migration fails prior to the VM being migrated this log will not be available. To access the log navigate to Completed Plans, and click **Download Log** and then **Migration log** next to the desired VM.
+
image::migration_log_access.png[Migration Log Access]

.. Once the log is downloaded click to open:
+
image::migration_log.png[Migration Log]


== Create an Infrastructure Mapping (vSphere to OpenStack)

. Navigate to the *Compute -> Migration -> Infrastructure Mappings*.
+
image::infrastructure_mapping_osp_1.png[Infrastructure Mapping 1]

. Click on *Create Infrastructure Mapping*.
+ 
image::infrastructure_mapping_osp_2.png[Infrastructure Mapping 2]

. In the *step 1* of the wizard, *General*, type the name `ticket-monster-map-osp`, select as *Target Provider* `Red Hat OpenStack Platform`, and click *next*.
+ 
image::infrastructure_mapping_osp_3.png[Infrastructure Mapping 3]
+ 
* A description may be added to make it easy to, later on, recognize the usage of the mapping.

. In the *step 2* of the wizard, *Clusters*, select *Source Cluster* as `vSphere\DC01\Cluster01` and *Target Provider\Project* as `OpenStack\ticket-monster` and click *Add Mapping*, then click *next*.
+ 
image::infrastructure_mapping_osp_4.png[Infrastructure Mapping 4]

. In the *step 3* of the wizard, *Datastores*, and having selected *Cluster01 (ticket-monster)* as the cluster to work with, select *Source Datastore* as `vSphere\Datastore` and *Target Datastore* as `OpenStack\default_store` and click *Add Mapping*, then click *next*.
+
image::infrastructure_mapping_osp_5.png[Infrastructure Mapping 5]

. In the *step 4* of the wizard, *Networks*, and having selected *Cluster01 (ticket-monster)* as the cluster to work with. We will start by mapping the network used by VMs to connect yto each other (i.e. JBoss EAP to the Database). We select *Source Network* as `vSphere\App-Internal-DPortGroup` and *Target Network* as `OpenStack\service` and click *Add Mapping*. 
+
image::infrastructure_mapping_osp_6a.png[Infrastructure Mapping 6]
+
* We will continue by mapping the network used by VMs to expose services to the internet(i.e. the Load Balancer exposing the Ticket Monster app). We select *Source Network* as `vSphere\App-DMZ-DPortGroup` and *Target Network* as `OpenStack\DMZ` and click *Add Mapping*.
+
image::infrastructure_mapping_osp_6b.png[Infrastructure Mapping 6]
+
* Finally we can map the management network. To do so, select *Source Network* as `vSphere\Management Network` and *Target Network* as `OpenStack\Admin` and click *Add Mapping*, then click *create*.
+ 
image::infrastructure_mapping_osp_6.png[Infrastructure Mapping 6]

. In the *step 5* of the wizard, *Results*, a message `All mappings in ticket-monster-map-osp have been mapped.` shall appear. Click *close*.
+ 
image::infrastructure_mapping_osp_7.png[Infrastructure Mapping 7]
+
image::infrastructure_mapping_osp_8.png[Infrastructure Mapping 8]

In these steps an *Infrastructure Mapping* has been created in order to simplify source and target resources using the data collected by Red hat CloudForms from both VMware vSphere and Red Hat OpenStack Platform.

== Migrating VMs to OSP with a Migration Plan

=== Create and run the migration plan

. Start in the CloudForms page accessed by navigating to *Compute -> Migration -> Migration Plans*.
+ 
image::migration_plan_0.png[Migration Plan 0]

. Click on *Create Migration Plan*.
+ 
image::migration_plan_osp_1.png[Migration Plan 1]

. In the *step 1* of the wizard, *General*, select in the drop down menu the *Infrastructure Mapping* to be used, `ticket-monster-osp`, add the name `ticket-monster-plan-lb` and click *next*.
+ 
image::migration_plan_osp_2.png[Migration Plan 2]

. In the *step 2* of the wizard, *VMs*, select the *lb* virtual machine, as the one to be migrated.
+ 
image::migration_plan_osp_3.png[Migration Plan 3]

. In the *step 3* of the wizard, *Instance Properties*, we can assign a specific OpenStack Flavor to the instance. We keep `m1.medium` for it and Click *Next*
+ 
image::migration_plan_osp_4.png[Migration Plan 4]

. In the *step 4* of the wizard, *Advanced Options*, we can assign *Pre* and *Post* migration playbooks to be executed during the migration. We won't use this feature just yet. Click *Next*
+ 
image::migration_plan_osp_5.png[Migration Plan 5]

. In the *step 5* of the wizard, *Schedule*, select *Start migration inmediately*. Click *Create*
+ 
image::migration_plan_osp_6.png[Migration Plan 6]

. In the *step 6* of the wizard, *Results*, the message `Migration Plan: ticket-monster-plan-app is in progress` shall appear. Click *Close*.
+
image::migration_plan_osp_7.png[Migration Plan 7]

. Back to the migration page we will see how the *Migration Plan* is already running
+
image::migration_plan_osp_8.png[Migration Plan 8]

. Once the migration is finished ...
+
image::migration_running_osp_1.png[Migration Running 1]

. We can find the instand in *OpenStack* in the *ticket-monster* project:
+
image::migration_running_osp_2.png[Migration Running 2]

== Running migrations using a CSV file

. Migrate the remaining application servers and database VMs from VMware to RHV using a CSV file.

.. Download the CSV sample file from link:../conf/ticket_monster_migration_sample.csv[here] and save it as `ticket_monster_migration_db.csv`. Check that the content is the following:
+
image:ticket_monster_csv_file.png[Multiple VM CSV File]
+
[TIP]
CSV file format is specified in the link:https://access.redhat.com/documentation/en-us/red_hat_infrastructure_migration_solution/1.0/html-single/infrastructure_migration_solution_guide/index#Creating_a_Migration_Plan[Official Documentation]

.. This sample file could be an example of a CMDB dump after processing. We may remove the lines we know we are not going to use, leaving only the header row and the `db` row. link:../conf/ticket_monster_migration_db.csv[Sample here].
+
image:ticket_monster_csv_db_file.png[Multiple VM CSV File]

.. Navigate to *Compute -> Migration -> Migration Plans* and click on *Create Migration Plan*. We will select the same infrastructure mapping previously created, `ticket-monster-mapping-rhv`, and select *Import CSV file with a list of VMs to be migrated*. Click *Next*.
+
image::csv_migration_plan_1.png[CSV Migration Plan 1]
+
image::csv_migration_plan_2.png[CSV Migration Plan 2]

.. In the *Import File* step, click on import and select the previously downloaded, and modified, file `ticket_monster_migration_db.csv`
+
image::csv_migration_plan_3.png[CSV Migration Plan 3]
+
image::csv_migration_plan_4.png[CSV Migration Plan 4]

.. The VM list will appear in the dialog. It is possible to modify the selection at this step but there is no need to. Click *Next*
+
image::csv_migration_plan_5.png[CSV Migration Plan 5]
+
[TIP]
A full migration can be done using the original sample file with a freshly instantiated environment.

.. In the *Advanced Options* step we will not apply any change. Click *Next*.
+
image::csv_migration_plan_6.png[CSV Migration Plan 6]

.. In the *Schedule* step we will *Save migration plan to run later*. The plan can later on be scheduled or directly run. Click *Next*.
+
image::csv_migration_plan_7.png[CSV Migration Plan 7]

.. *Results* page for the *Plan* will appear. Click *Close*
+
image::csv_migration_plan_8.png[CSV Migration Plan 8]

.. Back to the main *Migration Plans* page, we can schedule or run the plan created. Click *Schedule*
+
image::csv_migration_plan_9.png[CSV Migration Plan 9]

.. Choose date and time (i.e. two minutes from current time).
+
image::csv_migration_plan_10.png[CSV Migration Plan 10]
+
image::csv_migration_plan_11.png[CSV Migration Plan 11]
+
image::csv_migration_plan_12.png[CSV Migration Plan 12]

.. The plan will get automatically approved and start at the chosen time. 
+
image::csv_migration_plan_13.png[CSV Migration Plan 13]
+
image::csv_migration_plan_14.png[CSV Migration Plan 14]

.. Migration will take place
+
image::csv_migration_plan_15.png[CSV Migration Plan 15]
+
image::csv_migration_plan_16.png[CSV Migration Plan 16]

.. And VMs will be running in Red Hat Virtualization
+
image::csv_migration_plan_17.png[CSV Migration Plan 17]



.. Verify that the Ticket Monster app is running:

* Point your browser to https://app-<YOUR-GUID>.rhpds.opentlc.com and check it is running:
+
image::app-ticketmonster-running.png[Ticket Monster Web App]

If you want a deeper knowledge on how the whole Infrastructure Migration works, you may want to read the link:insfrastructure_migration-deployment_guide.adoc[Deployment guide]. 

